"""
Loader for CR3BP trajectory CSV files into PostgreSQL.

This module manages schema creation, foreign key relationships,
and the insertion of simulation metadata and time-series samples
into the database. All objects are created inside the schema
``cr3bp`` to keep the namespace isolated and consistent.

Tables created:

    • cr3bp.cr3bp_system
    • cr3bp.cr3bp_lagrange_point
    • cr3bp.cr3bp_simulation_run
    • cr3bp.cr3bp_trajectory_sample

The loader expects CSV files generated by the CR3BP batch exporter.
Each CSV corresponds to a single simulation run and contains
the full state time-series including position, velocity,
and acceleration components.
"""

from __future__ import annotations

import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Iterator

import pandas as pd
from psycopg2.extras import execute_values  # <--- WICHTIG: Für den Batch-Upload

from data_pipeline.load.db_connection import db_cursor
from sim_rl.cr3bp.scenarios import SCENARIOS


# Root where raw CSV batches are stored (mounted inside Docker)
RAW_EXPORT_ROOT = (Path(__file__).resolve().parent.parent / "raw_exports").resolve()


# ======================================================================
# 1) Schema Creation
# ======================================================================
def ensure_schema() -> None:
    """
    Create all CR3BP tables inside schema ``cr3bp`` if they do not exist.

    The structure mirrors the normalized relational layout used
    throughout the pipeline: system definitions, Lagrange points,
    simulation run metadata, and per-step trajectory samples.
    """

    ddl_system = """
    CREATE TABLE IF NOT EXISTS cr3bp.cr3bp_system (
        system_id     SERIAL PRIMARY KEY,
        name          TEXT UNIQUE NOT NULL,
        frame_default TEXT NOT NULL CHECK (
            frame_default IN ('rotating', 'inertial', 'barycentric')
        )
    );
    """

    ddl_lpoint = """
    CREATE TABLE IF NOT EXISTS cr3bp.cr3bp_lagrange_point (
        lagrange_point_id SERIAL PRIMARY KEY,
        system_id         INT NOT NULL REFERENCES cr3bp.cr3bp_system(system_id),
        name              TEXT NOT NULL,
        UNIQUE (system_id, name)
    );
    """

    ddl_run = """
    CREATE TABLE IF NOT EXISTS cr3bp.cr3bp_simulation_run (
        run_id                UUID PRIMARY KEY,
        system_id             INT NOT NULL REFERENCES cr3bp.cr3bp_system(system_id),
        lagrange_point_id     INT NOT NULL REFERENCES cr3bp.cr3bp_lagrange_point(lagrange_point_id),

        scenario_name         TEXT NOT NULL,
        dataset_tag           TEXT,
        created_at            TIMESTAMPTZ NOT NULL,
        source_file           TEXT NOT NULL,

        integrator            TEXT,
        step_size             DOUBLE PRECISION,
        terminated            BOOLEAN NOT NULL DEFAULT FALSE,
        termination_reason    TEXT,
        initial_condition_type TEXT
    );
    """

    ddl_sample = """
    CREATE TABLE IF NOT EXISTS cr3bp.cr3bp_trajectory_sample (
        run_id UUID NOT NULL REFERENCES cr3bp.cr3bp_simulation_run(run_id) ON DELETE CASCADE,
        step   INT NOT NULL,

        t      DOUBLE PRECISION NOT NULL,

        x      DOUBLE PRECISION NOT NULL,
        y      DOUBLE PRECISION NOT NULL,
        z      DOUBLE PRECISION NOT NULL,

        vx     DOUBLE PRECISION NOT NULL,
        vy     DOUBLE PRECISION NOT NULL,
        vz     DOUBLE PRECISION NOT NULL,

        ax     DOUBLE PRECISION NOT NULL,
        ay     DOUBLE PRECISION NOT NULL,
        az     DOUBLE PRECISION NOT NULL,

        PRIMARY KEY (run_id, step)
    );
    """

    with db_cursor(autocommit=True) as cur:
        cur.execute(ddl_system)
        cur.execute(ddl_lpoint)
        cur.execute(ddl_run)
        cur.execute(ddl_sample)


# ======================================================================
# 2) Metadata Helpers
# ======================================================================
def get_or_create_system(cur, name: str, frame_default: str = "rotating") -> int:
    """
    Retrieve or insert a CR3BP system entry.

    Parameters
    ----------
    name:
        Unique name of the primary system (for example ``"earth-moon"``).
    frame_default:
        Default reference frame used in associated simulations.
    """
    cur.execute(
        "SELECT system_id FROM cr3bp.cr3bp_system WHERE name = %s;",
        (name,),
    )
    row = cur.fetchone()
    if row:
        return row[0]

    cur.execute(
        """
        INSERT INTO cr3bp.cr3bp_system (name, frame_default)
        VALUES (%s, %s)
        RETURNING system_id;
        """,
        (name, frame_default),
    )
    return cur.fetchone()[0]


def get_or_create_lagrange_point(cur, system_id: int, name: str) -> int:
    """
    Retrieve or insert a Lagrange point entry.

    Parameters
    ----------
    system_id:
        Foreign key referencing ``cr3bp.cr3bp_system``.
    name:
        Name of the Lagrange point, for example ``"L1"``.
    """
    cur.execute(
        """
        SELECT lagrange_point_id
        FROM cr3bp.cr3bp_lagrange_point
        WHERE system_id = %s AND name = %s;
        """,
        (system_id, name),
    )
    row = cur.fetchone()
    if row:
        return row[0]

    cur.execute(
        """
        INSERT INTO cr3bp.cr3bp_lagrange_point (system_id, name)
        VALUES (%s, %s)
        RETURNING lagrange_point_id;
        """,
        (system_id, name),
    )
    return cur.fetchone()[0]


def parse_metadata_from_filename(csv_path: Path) -> Tuple[str, bool, str]:
    """
    Extract metadata from filenames of the form::

        traj_000_l1_cloud_max_steps.csv
        traj_013_halo_seed_escape.csv

    Returns
    -------
    ic_type:
        Extracted initial-condition classification string.
    terminated:
        Boolean indicating whether the run ended before ``max_steps``.
    termination_reason:
        The encoded termination label.
    """
    stem = csv_path.stem
    parts = stem.split("_")

    if len(parts) < 4 or parts[0] != "traj":
        raise ValueError(f"Unexpected trajectory filename: {csv_path.name!r}")

    termination_reason = parts[-1]
    ic_type = "_".join(parts[2:-1])
    terminated = termination_reason != "max_steps"
    return ic_type, terminated, termination_reason


# ======================================================================
# 3) Insert a Single CSV Run (OPTIMIZED)
# ======================================================================
def insert_run_and_trajectory(csv_path: Path, scenario_name: str) -> uuid.UUID:
    """
    Insert one trajectory CSV file into the CR3BP schema using optimized batch inserts.

    Parameters
    ----------
    csv_path:
        Path to the CSV file containing a trajectory.
    scenario_name:
        Scenario identifier referencing the ``SCENARIOS`` registry.

    Returns
    -------
    UUID
        The generated ``run_id`` representing this simulation.
    """
    if scenario_name not in SCENARIOS:
        raise KeyError(f"Scenario {scenario_name!r} not registered in SCENARIOS.")

    scenario = SCENARIOS[scenario_name]
    df = pd.read_csv(csv_path)

    required = {"t", "x", "y", "z", "vx", "vy", "vz", "ax", "ay", "az"}
    if not required.issubset(df.columns):
        raise ValueError(f"CSV {csv_path} missing required columns: {sorted(required)}")

    if len(df) < 2:
        raise ValueError(f"CSV {csv_path} contains too few samples.")

    step_size = float(df["t"].iloc[1] - df["t"].iloc[0])
    integrator = "DOP853"

    ic_type, terminated, termination_reason = parse_metadata_from_filename(csv_path)

    run_id = uuid.uuid4()
    created_at = datetime.utcnow()
    source_file = str(csv_path)

    dataset_tag = getattr(scenario, "dataset_tag", None)

    with db_cursor(autocommit=True) as cur:
        ensure_schema()

        system_id = get_or_create_system(cur, scenario.system, frame_default="rotating")
        lagrange_point_id = get_or_create_lagrange_point(
            cur,
            system_id,
            scenario.lagrange_point,
        )

        # 1. Insert Metadata (Single Row)
        cur.execute(
            """
            INSERT INTO cr3bp.cr3bp_simulation_run (
                run_id,
                system_id,
                lagrange_point_id,
                scenario_name,
                dataset_tag,
                created_at,
                source_file,
                integrator,
                step_size,
                terminated,
                termination_reason,
                initial_condition_type
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
            """,
            (
                str(run_id),  # psycopg2 handles UUIDs well, but string is safest
                system_id,
                lagrange_point_id,
                scenario_name,
                dataset_tag,
                created_at,
                source_file,
                integrator,
                step_size,
                terminated,
                termination_reason,
                ic_type,
            ),
        )

        # 2. Insert Trajectory Samples (BATCH INSERT via execute_values)
        # This is 100x faster than looping over cur.execute()
        
        insert_query = """
        INSERT INTO cr3bp.cr3bp_trajectory_sample (
            run_id, step, t, x, y, z, vx, vy, vz, ax, ay, az
        ) VALUES %s
        """

        # Prepare a list of tuples in memory
        # Adding run_id and step index to each row
        data_tuples = []
        for step, row in df.iterrows():
            data_tuples.append((
                str(run_id),
                int(step),
                float(row["t"]),
                float(row["x"]), float(row["y"]), float(row["z"]),
                float(row["vx"]), float(row["vy"]), float(row["vz"]),
                float(row["ax"]), float(row["ay"]), float(row["az"]),
            ))
            
        # Execute single large query
        execute_values(cur, insert_query, data_tuples)

    return run_id


# ======================================================================
# 4) File Discovery Utilities
# ======================================================================
def find_csv_files_for_date(
    scenario_name: str,
    date_str: Optional[str] = None,
) -> List[Path]:
    """
    Discover CSV files for a given scenario and date.

    Parameters
    ----------
    scenario_name:
        Name used in ``raw_exports/<scenario_name>/...``.
    date_str:
        Directory timestamp in ``YYYYMMDD`` format. If omitted, the
        current UTC date is used.

    Returns
    -------
    list of Path
        Ordered list of matching CSV trajectory files.
    """
    if date_str is None:
        date_str = datetime.utcnow().strftime("%Y%m%d")

    base_dir = RAW_EXPORT_ROOT / scenario_name / date_str
    if not base_dir.exists():
        return []

    subdirs = [d for d in base_dir.iterdir() if d.is_dir()]
    search_dir = sorted(subdirs)[-1] if subdirs else base_dir

    return sorted(search_dir.glob("traj_*.csv"))


# ======================================================================
# 5) High-Level Entry Points
# ======================================================================
def load_batch_for_date(
    scenario_name: str,
    date_str: Optional[str] = None,
) -> List[uuid.UUID]:
    """
    Load all CSV trajectory files for a single scenario and date.

    Parameters
    ----------
    scenario_name:
        Scenario identifier in the exporter directory.
    date_str:
        Date folder (``YYYYMMDD``). Defaults to the current UTC date.

    Returns
    -------
    list of UUID
        All inserted run identifiers corresponding to the scenario.
    """
    ensure_schema()
    csv_paths = find_csv_files_for_date(scenario_name, date_str)

    if not csv_paths:
        print(f"[INFO] No CSV files found for scenario={scenario_name}, date={date_str}")
        return []

    return [insert_run_and_trajectory(path, scenario_name) for path in csv_paths]


def load_raw_export_directory() -> List[uuid.UUID]:
    """
    Convenience entry point for orchestration tools.

    Loads all CSV trajectories for a configured list of scenarios
    on the current UTC date. This includes both standard L1/L1-cloud
    exports and halo-reference exports.

    Returns
    -------
    list of UUID
        Concatenated list of all inserted run identifiers.
    """
    today = datetime.utcnow().strftime("%Y%m%d")

    scenario_names = [
        "earth-moon-L1-3D",
        "earth-moon-L1-3D_halo_ref",
    ]

    all_ids: List[uuid.UUID] = []
    for scenario_name in scenario_names:
        ids = load_batch_for_date(scenario_name, today)
        all_ids.extend(ids)

    return all_ids


if __name__ == "__main__":
    ids = load_raw_export_directory()
    print(f"Inserted {len(ids)} simulation runs.")